{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0.0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports and declarations\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"image.cmap\"] = \"binary_r\"\n",
    "sys.path.append(\"../../semi-supervised\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Deep Generative Model\n",
    "\n",
    "In this notebook we will run the Auxiliary Deep Generative Model as described in (Maal√∏e 2016). The model is similar to the Deep Generative Model described in (Kingma 2014), with the addition of an auxiliary variable $a$. The idea is that introducing another latent variable that is marginalised out will allow the autoencoder to learn more complicated distributions and will be able to seperate the latent factors with higher precision.\n",
    "\n",
    "In the MNIST case, we can represent the variables as:\n",
    "\n",
    "* $y$: the label of the digit\n",
    "* $a$: the style of the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.limitedmnist import LimitedMNIST\n",
    "from utils import generate_label, onehot\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "n = len(labels)\n",
    "\n",
    "# Load in data\n",
    "mnist_lab = LimitedMNIST('./', train=True, transform=torch.bernoulli, target_transform=onehot(n), digits=labels, fraction=0.005)\n",
    "mnist_ulab = LimitedMNIST('./', train=True, transform=torch.bernoulli, target_transform=onehot(n), digits=labels, fraction=1.0)\n",
    "mnist_val = LimitedMNIST('./', train=False, transform=torch.bernoulli, target_transform=onehot(n), digits=labels)\n",
    "\n",
    "# Unlabelled data\n",
    "unlabelled = torch.utils.data.DataLoader(mnist_ulab, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Validation data\n",
    "validation = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Labelled data\n",
    "labelled = torch.utils.data.DataLoader(mnist_lab, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAB7CAYAAABQIQWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiBZhMERERESnwf59BWgCQcqXkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c572860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1, 10, figsize=(10, 2))\n",
    "images, labels = next(iter(labelled))\n",
    "_, labels = torch.max(labels, 1)\n",
    "\n",
    "for i in range(10):\n",
    "    axarr[i].imshow(images[i].numpy().reshape(28, 28))\n",
    "    title = labels[i]# if labels[i] < 5 else \"Unknown\"\n",
    "    axarr[i].set_title(title)\n",
    "    axarr[i].axis(\"off\")\n",
    "    \n",
    "f.suptitle(\"Data samples after Bernoulli transform\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again `VariationalInferenceWithLabels` is used to model the objective, shown below:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(x, y) &\\geq \\mathbb{E}_{q_{\\phi}(a, z|x, y)} [ \\log p_{\\theta}(x, y, a, z) - \\log q_{\\phi}(a, y, z|x) ]\\\\\n",
    "&= \\mathbb{E}_{q_{\\phi}(a, z|x, y)} [ \\log p_{\\theta}(x, y, a, z) - \\log q_{\\phi}(a | x) -  \\log q_{\\phi}(z | a, y, x) ] = - \\mathcal{L}(x, y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AuxiliaryDeepGenerativeModel (\n",
       "  (encoder): Encoder (\n",
       "    (hidden): ModuleList (\n",
       "      (0): Linear (16 -> 128)\n",
       "      (1): Linear (128 -> 64)\n",
       "    )\n",
       "    (sample): StochasticGaussian (\n",
       "      (mu): Linear (64 -> 16)\n",
       "      (log_var): Linear (64 -> 16)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder (\n",
       "    (hidden): ModuleList (\n",
       "      (0): Linear (16 -> 64)\n",
       "      (1): Linear (64 -> 128)\n",
       "    )\n",
       "    (reconstruction): Linear (128 -> 784)\n",
       "    (output_activation): Sigmoid ()\n",
       "  )\n",
       "  (aux_encoder): Encoder (\n",
       "    (hidden): ModuleList (\n",
       "      (0): Linear (784 -> 128)\n",
       "      (1): Linear (128 -> 64)\n",
       "    )\n",
       "    (sample): StochasticGaussian (\n",
       "      (mu): Linear (64 -> 16)\n",
       "      (log_var): Linear (64 -> 16)\n",
       "    )\n",
       "  )\n",
       "  (aux_decoder): Decoder (\n",
       "    (hidden): ModuleList (\n",
       "      (0): Linear (16 -> 64)\n",
       "      (1): Linear (64 -> 128)\n",
       "    )\n",
       "    (reconstruction): Linear (128 -> 16)\n",
       "    (output_activation): Sigmoid ()\n",
       "  )\n",
       "  (classifier): Classifier (\n",
       "    (dense): Linear (784 -> 128)\n",
       "    (logits): Linear (128 -> 10)\n",
       "  )\n",
       "  (transform_x_to_z): Linear (784 -> 16)\n",
       "  (transform_y_to_z): Linear (10 -> 16)\n",
       "  (transform_z_to_x): Linear (16 -> 784)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import AuxiliaryDeepGenerativeModel\n",
    "from inference.loss import VariationalInferenceWithLabels, kl_divergence_normal, discrete_uniform_prior\n",
    "\n",
    "# Numerical stability\n",
    "epsilon = 1e-7\n",
    "\n",
    "# ADGM with a single hidden layer in both the encoder and decoder\n",
    "model = AuxiliaryDeepGenerativeModel(ratio=len(mnist_ulab)/len(mnist_lab), dims=[28 * 28, n, 16, [128, 64]])\n",
    "\n",
    "if cuda: model.cuda()\n",
    "\n",
    "def binary_cross_entropy(r, x):\n",
    "    return torch.sum((x * torch.log(r + epsilon) + (1 - x) * torch.log((1 - r) + epsilon)), dim=-1)\n",
    "\n",
    "def cross_entropy(y, logits):\n",
    "    return -torch.sum(y * torch.log(logits + epsilon), dim=1)\n",
    "    \n",
    "objective = VariationalInferenceWithLabels(binary_cross_entropy, kl_divergence_normal, discrete_uniform_prior)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something about unlabelled lower bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMTrainer():\n",
    "    \"\"\"\n",
    "    Class for training Deep Generative Models.\n",
    "    :param model: Object of class `DeepGenerativeModel`\n",
    "    :param objective: Loss function for labelled data, e.g. `VariationalInferenceWithLabels`\n",
    "    :param optimizer: A PyTorch-enabled optimizer\n",
    "    :param cuda: Optional parameter whether to use CUDA acceleration\n",
    "    \"\"\"\n",
    "    def __init__(self, model, objective, optimizer, cuda=False):\n",
    "        self.model = model\n",
    "        self.objective = objective\n",
    "        self.optimizer = optimizer\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def calculate_loss(self, x, y=None):\n",
    "        \"\"\"\n",
    "        Given a semi-supervised problem (x, y) pair where y\n",
    "        is only occasionally observed, calculates the\n",
    "        associated loss.\n",
    "        :param x: Features\n",
    "        :param y: Labels (optional)\n",
    "        :returns L_alpha if labelled, U if unlabelled.\n",
    "        \"\"\"\n",
    "        is_unlabelled = True if y is None else False\n",
    "\n",
    "        x = Variable(x)\n",
    "        logits = self.model(x)\n",
    "\n",
    "        # If the data is unlabelled, sum over all classes\n",
    "        if is_unlabelled:\n",
    "            [batch_size, *_] = x.size()\n",
    "            x = x.repeat(n, 1)\n",
    "            y = torch.cat([generate_label(batch_size, i, n) for i in range(n)])\n",
    "\n",
    "        y = Variable(y.type(torch.FloatTensor))\n",
    "        \n",
    "        if self.cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        # Compute lower bound (the same as -L)\n",
    "        reconstruction, _, z, a = self.model(x, y)\n",
    "        ELBO = self.objective(reconstruction, x, y, [z, a])\n",
    "\n",
    "        # In the unlabelled case calculate the entropy H and return U\n",
    "        if is_unlabelled:\n",
    "            ELBO = ELBO.view(logits.size())\n",
    "            loss = torch.sum(torch.mul(logits, ELBO - torch.log(logits)), -1)\n",
    "            loss = -torch.mean(loss)\n",
    "        # In the case of labels add cross entropy and return L_alpha\n",
    "        else:\n",
    "            loss = ELBO + self.model.beta * -cross_entropy(y, logits)\n",
    "            loss = -torch.mean(loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train(self, labelled, unlabelled):\n",
    "        \"\"\"\n",
    "        Trains a DGM model based on some data.\n",
    "        :param labelled: Labelled data loader\n",
    "        :param inlabelled: Unlabelled data loader\n",
    "        :return L, U: Final loss values.\n",
    "        \"\"\"\n",
    "        for (x, y), (u, _) in zip(labelled, unlabelled):\n",
    "            L = self.calculate_loss(x, y)\n",
    "            U = self.calculate_loss(u, None)\n",
    "\n",
    "            J = L + U\n",
    "\n",
    "            J.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        return L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Here you can optionally enable the use of `visdom` to visualise the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_visdom = True\n",
    "\n",
    "if use_visdom:\n",
    "    import visdom\n",
    "    vis = visdom.Visdom()\n",
    "\n",
    "class Visualiser():\n",
    "    def __init__(self):\n",
    "        self.loss = vis.line(X=np.array([0]), Y=np.array([0]), opts=dict(title=\"Training Loss\", xlabel=\"Epoch\"))\n",
    "        self.acc  = vis.line(X=np.array([0]), Y=np.array([0]), opts=dict(title=\"Accuracy\", xlabel=\"Epoch\"))\n",
    "\n",
    "    def update_loss(self, L, U):\n",
    "        vis.updateTrace(X=np.array([epoch]), Y=L.data.numpy(), win=self.loss, name=\"Labelled\")\n",
    "        vis.updateTrace(X=np.array([epoch]), Y=U.data.numpy(), win=self.loss, name=\"Unlabelled\")\n",
    "        \n",
    "    def update_accuracy(self, model):\n",
    "        accuracy = []\n",
    "        for x, y in validation:\n",
    "            _, prediction = torch.max(model(Variable(x)), 1)\n",
    "            _, y = torch.max(y, 1)\n",
    "\n",
    "            accuracy += [torch.mean((prediction.data == y).float())]\n",
    "\n",
    "        vis.updateTrace(X=np.array([epoch]), Y=np.array([np.mean(accuracy)]), win=self.acc)\n",
    "        \n",
    "    def update_images(self, model):\n",
    "        x, y = next(iter(validation))\n",
    "        input = Variable(x[:5])\n",
    "        label = Variable(y[:5].type(torch.FloatTensor))\n",
    "        x_hat, *_ = model(input, label)\n",
    "        images = x_hat.data.numpy().reshape(-1, 1, 28, 28)\n",
    "\n",
    "        vis.images(images, opts=dict(width=5*64, height=64, caption=\"Sample epoch {}\".format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling which dataloaders to use, we gather the losses into a single combined loss that we can backpropagate.\n",
    "\n",
    "$$\\mathcal{J}^{\\alpha} = \\mathcal{L}^{\\alpha} + \\mathcal{U}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "trainer = DGMTrainer(model, objective, optimizer)\n",
    "visual = Visualiser()\n",
    "\n",
    "for epoch in range(1001):\n",
    "    L, U = trainer.train(labelled, unlabelled)\n",
    "        \n",
    "    if use_visdom:\n",
    "        # Plot the last L and U of the epoch\n",
    "        visual.update_loss(L, U)\n",
    "        visual.update_accuracy(model)\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            visual.update_images(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(torch.randn(16, 16))\n",
    "a = Variable(torch.randn(16, 16))\n",
    "y = Variable(generate_label(16, 0, 10))\n",
    "\n",
    "x = model.sample(z, a, y)\n",
    "\n",
    "images = x.view(-1, 28, 28).data.numpy()\n",
    "\n",
    "f, axarr = plt.subplots(4, 4, figsize=(4, 4))\n",
    "\n",
    "for i in range(16):\n",
    "    axarr[i%4, i//4].imshow(images[i])\n",
    "    axarr[i%4, i//4].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
